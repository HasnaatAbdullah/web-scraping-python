{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "prev_pub_hash": "c975bc569b09cdc49e9f499aac075862b2ff7356b23b8f06b39f154846eb837e"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# **Web Scraping**\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "!mamba install bs4==4.10.0 -y\n!pip install lxml==4.6.4\n!mamba install html5lib==1.1 -y\n!pip install pandas\n# !pip install requests==2.26.0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n,conda-forge/linux-64 ━━━━━━━━━━╸━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s━━━━━━━━━━━━  14.1kB /  20.2MB @ 180.9kB/s  0.0s\n,conda-forge/linux-64 ━╸━━━━━━━━━━━━━━━━━━━━━   4.4MB /  43.7MB @  24.2MB/s  0.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s━━━━━━━━━━━━   5.6MB /  20.2MB @  29.6MB/s  0.1s\n,conda-forge/linux-64 ━━━━╸━━━━━━━━━━━━━━━━━━   9.8MB /  43.7MB @  34.0MB/s  0.2s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s━━━━━━━━━━━━  10.5MB /  20.2MB @  35.8MB/s  0.2s\n,conda-forge/linux-64 ━━━━━━╸━━━━━━━━━━━━━━━━  15.2MB /  43.7MB @  38.7MB/s  0.3s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s━━━━━━╸━━━━━  16.0MB /  20.2MB @  40.1MB/s  0.3s\n,conda-forge/linux-64 ━━━━━━━━╸━━━━━━━━━━━━━━  17.9MB /  43.7MB @  40.0MB/s  0.4s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s━━━━━━━━╸━━━  18.4MB /  20.2MB @  41.0MB/s  0.4s\n,conda-forge/linux-64 ━━━━━━━━╸━━━━━━━━━━━━━━  17.9MB /  43.7MB @  40.0MB/s  0.5s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s━━━━━━━━╸━━━  18.4MB /  20.2MB @  41.0MB/s  0.5s\n,conda-forge/linux-64 ━━━━━━━━╸━━━━━━━━━━━━━━  17.9MB /  43.7MB @  40.0MB/s  0.6s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s━━━━━━━━╸━━━  18.4MB /  20.2MB @  41.0MB/s  0.6s\n,conda-forge/linux-64 ━━━━━━━━╸━━━━━━━━━━━━━━  17.9MB /  43.7MB @  40.0MB/s  0.7s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s━━━━━━━━╸━━━  18.4MB /  20.2MB @  41.0MB/s  0.7s\n,conda-forge/linux-64 ━━━━━━━━╸━━━━━━━━━━━━━━  17.9MB /  43.7MB @  40.0MB/s  0.8s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s━━━━━━━━╸━━━  18.4MB /  20.2MB @  41.0MB/s  0.8s\n,conda-forge/linux-64 ━━━━━━━━╸━━━━━━━━━━━━━━  17.9MB /  43.7MB @  40.0MB/s  0.9s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                  20.2MB @  41.0MB/s  1.0s\n,[+] 1.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s━━━━━━━━━╸━━━━━━━━━━  25.1MB /  43.7MB @  22.8MB/s  1.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s━━━━━━━━━━━╸━━━━━━━━  29.1MB /  43.7MB @  25.3MB/s  1.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s━━━━━━━━━━━━━━━━╸━━━  38.1MB /  43.7MB @  30.4MB/s  1.2s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  1.3s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  1.4s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  1.5s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  1.6s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.9s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  1.7s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.0s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  1.8s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.1s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  1.9s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.2s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.3s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.4s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.2s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.5s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.3s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.6s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.4s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.7s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.5s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.8s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.6s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.9s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.7s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.0s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.8s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.1s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  2.9s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.2s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.3s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.4s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.2s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.5s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.3s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.6s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.4s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.7s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.5s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.8s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.6s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.9s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.7s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.0s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.8s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.1s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  3.9s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.2s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.3s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.4s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.2s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.5s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.3s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.6s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.4s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.7s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.5s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.8s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.6s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.9s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.7s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.0s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.8s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.1s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  4.9s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.2s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.3s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.4s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.2s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.5s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.3s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.6s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.4s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.7s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.5s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.8s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.6s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 5.9s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.7s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.0s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.8s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.1s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  5.9s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.2s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.3s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.4s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.2s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.5s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.3s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.6s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.4s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.7s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.5s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.8s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.6s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 6.9s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.7s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.0s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.8s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.1s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  6.9s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.2s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  7.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 7.3s━━━━━━━━━━━━━━━━━╸━━  41.3MB /  43.7MB @  31.7MB/s  7.1s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                                43.7MB @  31.7MB/s  7.3s\n,DEPRECATION: --no-python-version-warning is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to remove the flag as it's a no-op. Discussion can be found at https://github.com/pypa/pip/issues/13154\n,\n,Pinned packages:\n,\n,  - python=3.12\n,\n,Pinned packages:\n,\n,  - python=3.12\n,\n,\n,Transaction\n,\n,  Prefix: /opt/conda\n,\n,  Updating specs:\n,\n,   - bs4==4.10.0\n,\n,\n,  Package           Version  Build         Channel          Size\n,──────────────────────────────────────────────────────────────────\n,  Install:\n,──────────────────────────────────────────────────────────────────\n,\n,  \u001b[32m+ bs4           \u001b[0m   4.10.0  hd8ed1ab_0    conda-forge       4kB\n,\n,  Downgrade:\n,──────────────────────────────────────────────────────────────────\n,\n,  \u001b[31m- beautifulsoup4\u001b[0m   4.12.3  pyha770c72_1  conda-forge     118kB\n,  \u001b[32m+ beautifulsoup4\u001b[0m   4.10.0  pyha770c72_0  conda-forge      79kB\n,\n,  Summary:\n,\n,  Install: 1 packages\n,  Downgrade: 1 packages\n,\n,  Total download: 84kB\n,\n,──────────────────────────────────────────────────────────────────\n,\n,\n,\n,Transaction starting\n,\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n,Downloading      \u001b[145m\u001b[170m-\u001b[0m\u001b[38;2;050;060;044m----------------------\u001b[0m   0.0 B                            0.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gbs4                                                  4.3kB @  62.4kB/s  0.0s.0s\n,beautifulsoup4                                      79.2kB @  ??.?MB/s  0.0s\n,[+] 0.1s\n,Downloading      \u001b[145m\u001b[170m-----------------------\u001b[0m  83.5kB                            0.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25hUnlinking beautifulsoup4-4.12.3-pyha770c72_1--------\u001b[0m       0 beautifulsoup4             0.0s\n,Linking beautifulsoup4-4.10.0-pyha770c72_0\n,Linking bs4-4.10.0-hd8ed1ab_0\n,\n,Transaction finished\n,\n,Collecting lxml==4.6.4\n,  Downloading lxml-4.6.4.tar.gz (3.2 MB)\n,\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n,  Preparing metadata (setup.py) ... \u001b[?25lerror\n,  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n,  \n,  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n,  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n,  \u001b[31m╰─>\u001b[0m \u001b[31m[5 lines of output]\u001b[0m\n,  \u001b[31m   \u001b[0m /tmp/pip-install-r48flc5b/lxml_6696af29e2724a2aab238dfa664f90dd/setup.py:67: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n,  \u001b[31m   \u001b[0m   import pkg_resources\n,  \u001b[31m   \u001b[0m Building lxml version 4.6.4.\n,  \u001b[31m   \u001b[0m Building without Cython.\n,  \u001b[31m   \u001b[0m Error: Please make sure the libxml2 and libxslt development packages are installed.\n,  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n,  \n,  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n,\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n,\n,\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n,\u001b[31m╰─>\u001b[0m See above for output.\n,\n,\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n,\u001b[1;36mhint\u001b[0m: See above for details.\n,conda-forge/linux-64                                        Using cache\n,conda-forge/noarch                                          Using cache\n,\u001b[?25l\u001b[2K\u001b[0G\u001b[?25hDEPRECATION: --no-python-version-warning is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to remove the flag as it's a no-op. Discussion can be found at https://github.com/pypa/pip/issues/13154\n,\n,Pinned packages:\n,\n,  - python=3.12\n,\n,Pinned packages:\n,\n,  - python=3.12\n,\n,\n,Transaction\n,\n,  Prefix: /opt/conda\n,\n,  Updating specs:\n,\n,   - html5lib==1.1\n,\n,\n,  Package     Version  Build         Channel         Size\n,───────────────────────────────────────────────────────────\n,  Install:\n,───────────────────────────────────────────────────────────\n,\n,  \u001b[32m+ html5lib\u001b[0m      1.1  pyhd8ed1ab_2  conda-forge     95kB\n,\n,  Summary:\n,\n,  Install: 1 packages\n,\n,  Total download: 95kB\n,\n,───────────────────────────────────────────────────────────\n,\n,\n,\n,Transaction starting\n,\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n,Downloading      -\u001b[00m\u001b[48;2;000;000;000m----------------------\u001b[0m   0.0 B                            0.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1sm\u001b[7m\u001b[8m\u001b[9m\u001b[38;2;254;065;026m\u001b[48;2;254;065;028m-----------------------\u001b[0m       0                            0.0s\n,Downloading      -----------------------   0.0 B                            0.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Ghtml5lib                                            94.9kB @  75.4kB/s  0.0s------\u001b[0m       0                            0.0s\n,[+] 0.2s\n,Downloading      -----------------------  94.9kB                            0.0s\n,\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25hLinking html5lib-1.1-pyhd8ed1ab_2m\u001b[48;2;254;065;028m-----\u001b[0m\u001b[2m\u001b[3m\u001b[4m\u001b[5m\u001b[7m\u001b[8m\u001b[9m\u001b[38;2;254;065;033m\u001b[48;2;254;065;035m----------------\u001b[0m\u001b[2m\u001b[3m\u001b[4m\u001b[5m\u001b[7m\u001b[8m\u001b[9m\u001b[38;2;254;065;026m\u001b[48;2;254;065;028m--\u001b[0m       0 html5lib                   0.0s\n,\n,Transaction finished\n,\n,Collecting pandas\n,  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n,Collecting numpy>=1.26.0 (from pandas)\n,  Downloading numpy-2.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n,Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n,Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n,Collecting tzdata>=2022.7 (from pandas)\n,  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n,Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n,Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n,\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m174.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n,\u001b[?25hDownloading numpy-2.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n,\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m153.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n,\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n,Installing collected packages: tzdata, numpy, pandas\n,Successfully installed numpy-2.2.5 pandas-2.2.3 tzdata-2025.2\n"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "Import the required modules and functions\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pip install --upgrade beautifulsoup4\n",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.12/site-packages (4.10.0)\n,Collecting beautifulsoup4\n,  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n,Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n,Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4) (4.12.2)\n,Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n,Installing collected packages: beautifulsoup4\n,  Attempting uninstall: beautifulsoup4\n,    Found existing installation: beautifulsoup4 4.10.0\n,    Uninstalling beautifulsoup4-4.10.0:\n,      Successfully uninstalled beautifulsoup4-4.10.0\n,Successfully installed beautifulsoup4-4.13.4\n,Note: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "pip install --upgrade pandas",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n,Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.5)\n,Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n,Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n,Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n,Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n,Note: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "suppress all warnings\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import warnings\nwarnings.simplefilter(\"ignore\")",
      "metadata": {},
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "from bs4 import BeautifulSoup # this module helps in web scrapping.\nimport requests  # this module helps us to download a web page",
      "metadata": {},
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "%%html\n<!DOCTYPE html>\n<html>\n<head>\n<title>Page Title</title>\n</head>\n<body>\n<h3><b id='boldest'>Lebron James</b></h3>\n<p> Salary: $ 92,000,000 </p>\n<h3> Stephen Curry</h3>\n<p> Salary: $85,000, 000 </p>\n<h3> Kevin Durant </h3>\n<p> Salary: $73,200, 000</p>\n</body>\n</html>",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can store it as a string in the variable HTML:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "html=\"<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "To parse a document, pass it into the <code>BeautifulSoup</code> constructor, the <code>BeautifulSoup</code> object, which represents the document as a nested data structure:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "soup = BeautifulSoup(html, \"html.parser\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "First, the document is converted to Unicode, (similar to ASCII),  and HTML entities are converted to Unicode characters. Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. The <code>BeautifulSoup</code> object can create other types of objects. In this lab, we will cover <code>BeautifulSoup</code> and <code>Tag</code> objects that for the purposes of this lab are identical, and <code>NavigableString</code> objects.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can use the method <code>prettify()</code> to display the HTML in the nested structure:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(soup.prettify())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Tags\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's say we want the  title of the page and the name of the top paid player we can use the <code>Tag</code>. The <code>Tag</code> object corresponds to an HTML tag in the original document, for example, the tag title.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag_object=soup.title\nprint(\"tag object:\",tag_object)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "we can see the tag type <code>bs4.element.Tag</code>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"tag object type:\",type(tag_object))",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "If there is more than one <code>Tag</code>  with the same name, the first element with that <code>Tag</code> name is called, this corresponds to the most paid player:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag_object=soup.h3\ntag_object",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Enclosed in the bold attribute <code>b</code>, it helps to use the tree representation. We can navigate down the tree using the child attribute to get the name.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Children, Parents, and Siblings\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "As stated above the <code>Tag</code> object is a tree of objects we can access the child of the tag or navigate down the branch as follows:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag_child =tag_object.b\ntag_child",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "You can access the parent with the <code> parent</code>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "parent_tag=tag_child.parent\nparent_tag",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "this is identical to\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag_object",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<code>tag_object</code> parent is the <code>body</code> element.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag_object.parent",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<code>tag_object</code> sibling is the <code>paragraph</code> element\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sibling_1=tag_object.next_sibling\nsibling_1",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "`sibling_2` is the `header` element which is also a sibling of both `sibling_1` and `tag_object`\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sibling_2=sibling_1.next_sibling\nsibling_2",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "sibling_2.next_sibling",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "If the tag has attributes, the tag <code>id=\"boldest\"</code> has an attribute <code>id</code> whose value is <code>boldest</code>. You can access a tag’s attributes by treating the tag like a dictionary:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag_child['id']",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "You can access that dictionary directly as <code>attrs</code>:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag_child.attrs",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "You can also work with Multi-valued attribute check out <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01\">\\[1]</a> for more.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can also obtain the content if the attribute of the <code>tag</code> using the Python <code>get()</code> method.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag_child.get('id')",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "tag_string=tag_child.string\ntag_string",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "we can verify the type is Navigable String\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "type(tag_string)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "A NavigableString is just like a Python string or Unicode string, to be more precise. The main difference is that it also supports some  <code>BeautifulSoup</code> features. We can covert it to sting object in Python:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "unicode_string = str(tag_string)\nunicode_string",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "%%html\n<table>\n  <tr>\n    <td id='flight' >Flight No</td>\n    <td>Launch site</td> \n    <td>Payload mass</td>\n   </tr>\n  <tr> \n    <td>1</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n    <td>300 kg</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n    <td>94 kg</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td>\n    <td>80 kg</td>\n  </tr>\n</table>",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "table=\"<table><tr><td id='flight' >Flight No</td><td>Launch site</td><td>Payload mass</td></tr><tr><td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td><td>80 kg</td></tr></table>\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "table_bs = BeautifulSoup(table, \"html.parser\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "When we set the <code>name</code> parameter to a tag name, the method will extract all the tags with that name and its children.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "table_rows=table_bs.find_all('tr')\ntable_rows",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The result is a Python Iterable just like a list, each element is a <code>tag</code> object:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "first_row =table_rows[0]\nfirst_row",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The type is <code>tag</code>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(type(first_row))",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "we can obtain the child\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "first_row.td",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "If we iterate through the list, each element corresponds to a row in the table:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for i,row in enumerate(table_rows):\n    print(\"row\",i,\"is\",row)\n    ",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "As <code>row</code> is a <code>cell</code> object, we can apply the method <code>find_all</code> to it and extract table cells in the object <code>cells</code> using the tag <code>td</code>, this is all the children with the name <code>td</code>. The result is a list, each element corresponds to a cell and is a <code>Tag</code> object, we can iterate through this list as well. We can extract the content using the <code>string</code>  attribute.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for i,row in enumerate(table_rows):\n    print(\"row\",i)\n    cells=row.find_all('td')\n    for j,cell in enumerate(cells):\n        print('colunm',j,\"cell\",cell)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "If we use a list we can match against any item in that list.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "list_input=table_bs .find_all(name=[\"tr\", \"td\"])\nlist_input",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "table_bs.find_all(id=\"flight\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "list_input=table_bs.find_all(href=\"https://en.wikipedia.org/wiki/Florida\")\nlist_input",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "table_bs.find_all(href=True)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "There are other methods for dealing with attributes and other related methods; Check out the following <a href='https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01#css-selectors'>link</a>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Using the logic above, find all the elements without <code>href</code> value\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "table_bs.find_all('a', href=False)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Using the soup object <code>soup</code>, find the element with the <code>id</code> attribute content set to <code>\"boldest\"</code>.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "soup.find_all(id=\"boldest\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "table_bs.find_all(string=\"Florida\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "%%html\n<h3>Rocket Launch </h3>\n\n<p>\n<table class='rocket'>\n  <tr>\n    <td>Flight No</td>\n    <td>Launch site</td> \n    <td>Payload mass</td>\n  </tr>\n  <tr>\n    <td>1</td>\n    <td>Florida</td>\n    <td>300 kg</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td>Texas</td>\n    <td>94 kg</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td>Florida </td>\n    <td>80 kg</td>\n  </tr>\n</table>\n</p>\n<p>\n\n<h3>Pizza Party  </h3>\n  \n    \n<table class='pizza'>\n  <tr>\n    <td>Pizza Place</td>\n    <td>Orders</td> \n    <td>Slices </td>\n   </tr>\n  <tr>\n    <td>Domino's Pizza</td>\n    <td>10</td>\n    <td>100</td>\n  </tr>\n  <tr>\n    <td>Little Caesars</td>\n    <td>12</td>\n    <td >144 </td>\n  </tr>\n  <tr>\n    <td>Papa John's </td>\n    <td>15 </td>\n    <td>165</td>\n  </tr>\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We store the HTML as a Python string and assign <code>two_tables</code>:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "two_tables=\"<h3>Rocket Launch </h3><p><table class='rocket'><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table></p><p><h3>Pizza Party  </h3><table class='pizza'><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td >144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr>\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We create a <code>BeautifulSoup</code> object  <code>two_tables_bs</code>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "two_tables_bs= BeautifulSoup(two_tables, 'html.parser')",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can find the first table using the tag name table\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "two_tables_bs.find(\"table\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can filter on the class attribute to find the second table, but because class is a keyword in Python, we add an underscore.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "two_tables_bs.find(\"table\",class_='pizza')",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We Download the contents of the web page:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "url = \"https://web.archive.org/web/20230224123642/https://www.ibm.com/us-en/\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We use <code>get</code> to download the contents of the webpage in text format and store in a variable called <code>data</code>:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data  = requests.get(url).text ",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We create a <code>BeautifulSoup</code> object using the <code>BeautifulSoup</code> constructor\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "soup = BeautifulSoup(data,\"html.parser\")  # create a soup object using the variable 'data'",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Scrape all links\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for link in soup.find_all('a',href=True):  # in html anchor/link is represented by the tag <a>\n\n    print(link.get('href'))\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "for link in soup.find_all('img'):# in html image is represented by the tag <img>\n    print(link)\n    print(link.get('src'))",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#The below url contains an html table with data about colors and color codes.\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Before proceeding to scrape a web site, you need to examine the contents, and the way data is organized on the website. Open the above url in your browser and check how many rows and columns are there in the color table.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# get the contents of the webpage in text format and store in a variable called data\ndata  = requests.get(url).text",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "soup = BeautifulSoup(data,\"html.parser\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#find a html table in the web page\ntable = soup.find('table') # in html table is represented by the tag <table>",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Get all rows from the table\nfor row in table.find_all('tr'): # in html table row is represented by the tag <tr>\n    # Get all columns in each row.\n    cols = row.find_all('td') # in html a column is represented by the tag <td>\n    color_name = cols[2].string # store the value in column 3 as color_name\n    color_code = cols[3].string # store the value in column 4 as color_code\n    print(\"{}--->{}\".format(color_name,color_code))",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#The below url contains html tables with data about world population.\nurl = \"https://en.wikipedia.org/wiki/World_population\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Before proceeding to scrape a web site, you need to examine the contents, and the way data is organized on the website. Open the above url in your browser and check the tables on the webpage.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# get the contents of the webpage in text format and store in a variable called data\ndata  = requests.get(url).text",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "soup = BeautifulSoup(data,\"html.parser\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#find all html tables in the web page\ntables = soup.find_all('table') # in html table is represented by the tag <table>",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# we can see how many tables were found by checking the length of the tables list\nlen(tables)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "for index,table in enumerate(tables):\n    if (\"10 most densely populated countries\" in str(table)):\n        table_index = index\nprint(table_index)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "See if you can locate the table name of the table, `10 most densly populated countries`, below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(tables[table_index].prettify())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "population_data = pd.DataFrame(columns=[\"Rank\", \"Country\", \"Population\", \"Area\", \"Density\"])\n\nfor row in tables[table_index].tbody.find_all(\"tr\"):\n    col = row.find_all(\"td\")\n    if col:\n        rank = col[0].text.strip()\n        country = col[1].text.strip()\n        population = col[2].text.strip()\n        area = col[3].text.strip()\n        density = col[4].text.strip()\n\n        # Create a temporary DataFrame for the new row\n        new_row = pd.DataFrame([{\"Rank\": rank, \"Country\": country, \"Population\": population, \"Area\": area, \"Density\": density}])\n\n        # Use concat \n        population_data = pd.concat([population_data, new_row], ignore_index=True)\n\npopulation_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Scrape data from HTML tables into a DataFrame using BeautifulSoup and read_html\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Using the same `url`, `data`, `soup`, and `tables` object as in the last section we can use the `read_html` function to create a DataFrame.\n\nRemember the table we need is located in `tables[table_index]`\n\nWe can now use the `pandas` function `read_html` and give it the string version of the table as well as the `flavor` which is the parsing engine `bs4`.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pd.read_html(str(tables[5]), flavor='bs4')",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The function `read_html` always returns a list of DataFrames so we must pick the one we want out of the list.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "population_data_read_html = pd.read_html(str(tables[5]), flavor='bs4')[0]\n\npopulation_data_read_html",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Scrape data from HTML tables into a DataFrame using read_html\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can also use the `read_html` function to directly get DataFrames from a `url`.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dataframe_list = pd.read_html(url, flavor='bs4')",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can see there are 25 DataFrames just like when we used `find_all` on the `soup` object.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "len(dataframe_list)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Finally we can pick the DataFrame we need out of the list.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dataframe_list[5]",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We can also use the `match` parameter to select the specific table we want. If the table contains a string matching the text it will be read.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pd.read_html(url, match=\"10 most densely populated countries\", flavor='bs4')[0]",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}